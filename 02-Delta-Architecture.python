{"version":"NotebookV1","origId":17383883,"name":"02-Delta-Architecture","language":"python","commands":[{"version":"CommandV1","origId":17383884,"guid":"c2635e03-6a30-4293-8b04-b5da86c5fd0a","subtype":"command","commandType":"auto","position":2.0,"command":"%md-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"63d3af0b-a0f3-4a66-91ad-0e58e5c524ed"},{"version":"CommandV1","origId":17383885,"guid":"b66b0c65-2d31-491b-97c0-36d997c1c2ee","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n# <img src=\"https://files.training.databricks.com/images/DeltaLake-logo.png\" width=80px> Delta Lake Architecture\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Discuss the advantages of Delta over a traditional Lambda architecture\n* Describe Bronze, Silver, and Gold tables\n* Create a Delta Lake pipeline\n\nThis notebook demonstrates using Delta Lakes as an optimization layer on top of cloud-based object storage to ensure reliability (i.e. ACID compliance) and low latency within unified Streaming + Batch data pipelines.","commandVersion":14,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"01bd64fc-56c6-4b99-a752-433dcab1ce8d"},{"version":"CommandV1","origId":17383886,"guid":"f8923bc2-7a02-4df8-be63-25197d50cf40","subtype":"command","commandType":"auto","position":4.0,"command":"%md-sandbox\n## Lambda Architecture\n\nThe Lambda architecture is a big data processing architecture that combines both batch- and real-time processing methods.\nIt features an append-only immutable data source that serves as system of record. Timestamped events are appended to\nexisting events (nothing is overwritten). Data is implicitly ordered by time of arrival.\n\nNotice how there are really two pipelines here, one batch and one streaming, hence the name <i>lambda</i> architecture.\n\nIt is very difficult to combine processing of batch and real-time data as is evidenced by the diagram below.\n\n\n<div><img src=\"https://files.training.databricks.com/images/adbcore/delta_arch/lambda_arch_slide.png\" style=\"width: 800px\"/></div><br/>","commandVersion":21,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"362dbb05-da9f-4aa1-9694-61946c79fd85"},{"version":"CommandV1","origId":17383887,"guid":"f3dc9fa4-a3c9-474e-97ba-72ceefdf8cbe","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n## Delta Lake Architecture\n\nThe Delta Lake Architecture is a vast improvmemt upon the traditional Lambda architecture. At each stage, we enrich our data through a unified pipeline that allows us to combine batch and streaming workflows through a shared filestore with ACID compliant transactions.\n\n- **Bronze** tables contain raw data ingested from various sources (JSON files, RDBMS data,  IoT data, etc.).\n\n- **Silver** tables will provide a more refined view of our data. We can join fields from various bronze tables to enrich streaming records, or update account statuses based on recent activity.\n\n- **Gold** tables provide business level aggregates often used for reporting and dashboarding. This would include aggregations such as daily active website users, weekly sales per store, or gross revenue per quarter by department. \n\nThe end outputs are actionable insights, dashboards and reports of business metrics.\n\n**Today, we will focus on implementing the multi-hop pipeline outlined in red.**\n\n<img src=https://files.training.databricks.com/images/adbcore/delta_arch/delta_arch_slide_box.png width=800px>\n\n\nBy considering our business logic at all steps of the ETL pipeline, we can ensure that storage and compute costs are optimized by reducing unnecessary duplication of data and limiting ad hoc querying against full historic data.\n\nEach stage can be configured as a batch or streaming job, and ACID transactions ensure that we succeed or fail completely.","commandVersion":44,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"a80c3525-9a52-4f66-836c-651d620de3c0"},{"version":"CommandV1","origId":17383888,"guid":"87e157d8-5af2-4ae6-a23e-2691598e0661","subtype":"command","commandType":"auto","position":5.5,"command":"%md\n## Benefits of the Delta Architecture\n\n1. Reduce end-to-end pipeline SLA.\n  - Organizations reduced pipeline SLAs from days and hours to minutes.\n1. Reduce pipeline maintenance burden.\n  - Eliminate lambda architectures for minute-latency use cases.\n1. Handle updates and deletes easily.\n  - Change data capture, GDPR, Sessionization, Deduplication use cases simplified.\n1. Lower infrastructure costs with elastic, independent compute & storage.\n  - Organizations reduce infrastructure costs by up to 10x.\n","commandVersion":25,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"1655b52f-66a7-41b5-a679-c38ffcc69716"},{"version":"CommandV1","origId":17383889,"guid":"9cfce840-3cdd-4b31-9010-81b0a79b2b54","subtype":"command","commandType":"auto","position":6.0,"command":"%md\n\n<img src=\"https://files.training.databricks.com/images/DeltaLake-logo.png\" width=\"80px\"/>\n\n# Unifying Structured Streaming with Batch Jobs with Delta Lake\n\nIn this notebook, we will explore combining streaming and batch processing with a single pipeline. We will begin by defining the following logic:\n\n- ingest streaming JSON data from disk and write it to a Delta Lake Table `/activity/Bronze`\n- perform a Stream-Static Join on the streamed data to add additional geographic data\n- transform and load the data, saving it out to our Delta Lake Table `/activity/Silver`\n- summarize the data through aggregation into the Delta Lake Table `/activity/Gold/groupedCounts`\n- materialize views of our gold table through streaming plots and static queries\n\nWe will then demonstrate that by writing batches of data back to our bronze table, we can trigger the same logic on newly loaded data and propagate our changes automatically.","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"cd9121c6-74d7-44f2-ba08-13ebef9f4287"},{"version":"CommandV1","origId":17383890,"guid":"be12750c-35fd-4f36-803a-eabbd8cbec04","subtype":"command","commandType":"auto","position":6.5,"command":"%md\n## Notebook Setup\n\nCreate a database (to not conflict with other databases in the workspace).","commandVersion":31,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"031037d1-2992-4f4f-880e-840f9fef472a"},{"version":"CommandV1","origId":17383891,"guid":"e6c1c9ac-2d9b-40de-bbcd-6d4f0baa5528","subtype":"command","commandType":"auto","position":7.0,"command":"%sql\nCREATE DATABASE IF NOT EXISTS dbacademy;\nUSE dbacademy;","commandVersion":6,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002049170,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2f4e9f7e-f312-4d0d-8a8e-27ee13141eb4"},{"version":"CommandV1","origId":17383892,"guid":"11543d2a-605b-4602-b77d-47d8e455ee94","subtype":"command","commandType":"auto","position":8.0,"command":"%md\nFor small data, we can get a modest performance gain by setting shuffle partitions to match the number of executor CPU cores in the cluster. (The setting 2 works well for Databricks Community Edition.)  For larger data, you likely will want to keep the default of 200.","commandVersion":36,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"92aae56b-0796-4b27-aa6d-d3faee30087f"},{"version":"CommandV1","origId":17383893,"guid":"d1a55297-26fd-43d0-b1bd-6af0dfc21399","subtype":"command","commandType":"auto","position":9.0,"command":"sqlContext.setConf(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)","commandVersion":9,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002050864,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"b5481344-9ad7-4dee-91d8-75a1bb362d31"},{"version":"CommandV1","origId":17383894,"guid":"5552c06c-5449-4a5f-99be-46416fec42b7","subtype":"command","commandType":"auto","position":10.0,"command":"%md-sandbox\n## Set up relevant Delta Lake paths\n\nThese paths will serve as the file locations for our Delta Lake tables.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Each streaming write has its own checkpoint directory.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> You cannot write out new Delta files within a repository that contains Delta files. Note that our hierarchy here isolates each Delta table into its own directory.","commandVersion":6,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"b0941a4e-e579-4ea6-8c1e-05caff7c0102"},{"version":"CommandV1","origId":17383895,"guid":"0ad3dd16-59b7-462b-9c88-cefa39b41733","subtype":"command","commandType":"auto","position":11.0,"command":"source_dir = \"s3a://databricks-corp-training/common/healthcare/\"\n\nbasePath = \"dbfs:/dbacademy/streaming-delta\"\n\nstreamingPath          = basePath + \"/source\"\nbronzePath             = basePath + \"/bronze\"\nrecordingsParsedPath   = basePath + \"/silver/recordings_parsed\"\nrecordingsEnrichedPath = basePath + \"/silver/recordings_enriched\"\ndailyAvgPath           = basePath + \"/gold/dailyAvg\"\n\ncheckpointPath               = basePath + \"/checkpoints\"\nbronzeCheckpoint             = basePath + \"/checkpoints/bronze\"\nrecordingsParsedCheckpoint   = basePath + \"/checkpoints/recordings_parsed\"\nrecordingsEnrichedCheckpoint = basePath + \"/checkpoints/recordings_enriched\"\ndailyAvgCheckpoint           = basePath + \"/checkpoints/dailyAvgPath\"","commandVersion":410,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002098192,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"4e33fbb1-6c73-462d-bf0f-db9f636c982e"},{"version":"CommandV1","origId":17383896,"guid":"fac81cf9-5cd0-4734-84cf-102cf0bf20f0","subtype":"command","commandType":"auto","position":12.0,"command":"%md\n## Reset Pipeline\n\nTo clear out old files from prior runs of this notebook, run the following:","commandVersion":11,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"f9c501be-c62f-4802-b0e0-ee4779431d15"},{"version":"CommandV1","origId":17383897,"guid":"55d35174-6e0e-46f0-9e99-58be5759dac1","subtype":"command","commandType":"auto","position":13.0,"command":"dbutils.fs.rm(basePath, True)","commandVersion":5,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002057034,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"99548262-6b21-4816-a53d-513cf96715e5"},{"version":"CommandV1","origId":17383898,"guid":"d32f4023-8842-43a7-a00d-ad7fd646e32d","subtype":"command","commandType":"auto","position":14.0,"command":"%md-sandbox\n## Datasets Used\n\nThis demo uses simplified (and artificially generated) medical data. The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n\n#### Recordings\nThe main dataset uses heart rate recordings from medical devices delivered in the JSON format. \n\n| Field | Type |\n| --- | --- |\n| device_id | int |\n| mrn | long |\n| time | double |\n| heartrate | double |\n\n#### PII\nThese data will later be joined with a static table of patient information stored in an external system to identify patients by name.\n\n| Field | Type |\n| --- | --- |\n| mrn | long |\n| name | string |","commandVersion":407,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"3ff30fe4-3b71-47ff-9a94-0f9eec5ce35c"},{"version":"CommandV1","origId":17383899,"guid":"673415e1-f000-4b63-8b9b-369ca897753d","subtype":"command","commandType":"auto","position":14.25,"command":"%md\n## Data Simulator\nSpark Structured Streaming can automatically process files as they land in your cloud object stores. To simulate this process, you will be asked to run the following operation several times throughout the course.","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"60ce7d9e-99a9-499b-9337-160436aa9edb"},{"version":"CommandV1","origId":17383900,"guid":"783b56c2-e5e9-40a6-857e-0efd6094fecb","subtype":"command","commandType":"auto","position":14.375,"command":"class FileArrival:\n  def __init__(self):\n    self.source = source_dir + \"/tracker/streaming/\"\n    self.userdir = streamingPath + \"/\"\n    self.curr_mo = 1\n    \n  def arrival(self, continuous=False):\n    if self.curr_mo > 12:\n      print(\"Data source exhausted\\n\")\n    elif continuous == True:\n      while self.curr_mo <= 12:\n        curr_file = f\"{self.curr_mo:02}.json\"\n        dbutils.fs.cp(self.source + curr_file, self.userdir + curr_file)\n        self.curr_mo += 1\n    else:\n      curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n      dbutils.fs.cp(self.source + curr_file, self.userdir + curr_file)\n      self.curr_mo += 1\n      \nNewFile = FileArrival()","commandVersion":186,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002101123,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"dfef98cc-21ab-4701-a83f-1878b57255b5"},{"version":"CommandV1","origId":17383901,"guid":"b3a703f3-4b9f-421c-b638-7478c478cd95","subtype":"command","commandType":"auto","position":14.4375,"command":"NewFile.arrival()\ndisplay(dbutils.fs.ls(streamingPath))","commandVersion":15,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002103145,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"4835704e-16ae-4d77-b8be-2eabff48b64a"},{"version":"CommandV1","origId":17383902,"guid":"10d0768e-c1d3-4b8c-a3c1-22138bf1d121","subtype":"command","commandType":"auto","position":14.5,"command":"%md\n## Bronze Table: Ingesting Raw JSON Recordings\n\nNote that we'll be keeping our data in its raw format during this stage by reading our JSON as a text file. In this way, we ensure that all data will make it into our bronze Delta table. If any of our records are corrupted or have different schema, we can build downstream logic to decide how to handle these exceptions.","commandVersion":209,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"16c5fe7f-286b-4a44-a97e-9ab426a99ac6"},{"version":"CommandV1","origId":17383903,"guid":"7c7b8fad-4082-4c64-a805-daa2a758b73a","subtype":"command","commandType":"auto","position":14.625,"command":"%md\n### Read Stream\nNote that while you need to use the Spark DataFrame API to set up a streaming read, once configured you can immediately register a temp view to leverage Spark SQL for streaming transformations on your data.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"91ee429d-e96d-480f-89e2-bfd30edab078"},{"version":"CommandV1","origId":17383904,"guid":"0a05af53-13ff-480d-8a21-35097b44e514","subtype":"command","commandType":"auto","position":14.75,"command":"(spark.readStream\n  .format(\"text\")\n  .schema(\"data STRING\")\n  .option(\"maxFilesPerTrigger\", 1)  # This is used for testing to simulate 1 file arriving at a time.  Generally, don't set this in production.\n  .load(streamingPath)\n  .createOrReplaceTempView(\"recordings_raw_temp\"))","commandVersion":125,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002114789,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"081f6455-b366-416c-b1b0-0d35d129c099"},{"version":"CommandV1","origId":17383905,"guid":"ee2275ac-992d-4e17-a452-31ef31a69b29","subtype":"command","commandType":"auto","position":15.5625,"command":"%md\nEncoding the receipt time and the name of our dataset would allow us to use a single bronze table for multiple different data sources. This multiplex table design replicates the semi-structured nature of data stored in most data lakes while guaranteeing ACID transactions.\n\nDownstream, we'll be able to subscribe to this table using the `dataset` field as a predicate, giving us a single table with read-after-write consistency guarantees as a source for multiple different queries.","commandVersion":183,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"1fe44bdd-baf9-4762-8df1-6203bcf9318e"},{"version":"CommandV1","origId":17383906,"guid":"e257be3f-417e-405e-a88b-51a300eaf3c8","subtype":"command","commandType":"auto","position":16.375,"command":"%sql\nCREATE OR REPLACE TEMPORARY VIEW recordings_bronze_temp AS (\n  SELECT current_timestamp() receipt_time, \"recordings\" dataset, *\n  FROM recordings_raw_temp\n)","commandVersion":50,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002136173,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"762d54e0-8893-4c14-b057-7d5b6499308b"},{"version":"CommandV1","origId":17383907,"guid":"0d3fc3f7-45ef-44de-a72b-25521a11df5c","subtype":"command","commandType":"auto","position":18.09375,"command":"%md-sandbox\n### Write Stream using Delta Lake\n\n#### General Notation\nUse this format to write a streaming job to a Delta Lake table.\n\n<pre>\n(myDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath)\n  .outputMode(\"append\")\n  .start(path)\n)\n</pre>\n\n#### Output Modes\nNotice, besides the \"obvious\" parameters, specify `outputMode`, which can take on these values\n* `append`: add only new records to output sink\n* `complete`: rewrite full output - applicable to aggregations operations\n\n#### Checkpointing\n\n- When defining a Delta Lake streaming query, one of the options that you need to specify is the location of a checkpoint directory.\n`.option(\"checkpointLocation\", \"/path/to/checkpoint/directory/\")`\n- This is actually a structured streaming feature. It stores the current state of your streaming job. Should your streaming job stop for some reason and you restart it, it will continue from where it left off.\n- If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n- Also note that every streaming job should have its own checkpoint directory: no sharing.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"bf16d818-4c3e-4080-b80a-9d6f7a9c2bb8"},{"version":"CommandV1","origId":17383908,"guid":"e320b75c-588c-441f-bf41-6571f727140c","subtype":"command","commandType":"auto","position":19.0,"command":"(spark.table(\"recordings_bronze_temp\")\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", bronzeCheckpoint)\n  .outputMode(\"append\")\n  .start(bronzePath))","commandVersion":25,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002142373,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"47e56c79-fc43-4edb-a5f6-0e86c65d4ad3"},{"version":"CommandV1","origId":17383909,"guid":"807f6359-ac14-4f67-8dbd-d9ba413fb911","subtype":"command","commandType":"auto","position":19.25,"command":"%md\nTrigger another file arrival with the following cell and you'll see the changes immediately detected by the streaming query you've written.","commandVersion":28,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"0fc89363-5ee3-41c6-a10d-f0d26ccf6904"},{"version":"CommandV1","origId":17383910,"guid":"d89bc4b5-99d2-4887-9add-9b7b8fedc79d","subtype":"command","commandType":"auto","position":19.3125,"command":"# Display how many records are in our table so we can watch it grow.\ndisplay(spark.readStream.format(\"delta\").load(bronzePath).groupBy().count())","commandVersion":14,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002471052,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"e719ed3b-00fb-413f-bf53-6f3732830610"},{"version":"CommandV1","origId":17383911,"guid":"781a3ba5-a8bd-449d-a910-f75b06e93059","subtype":"command","commandType":"auto","position":19.375,"command":"NewFile.arrival()","commandVersion":2,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002148402,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"78b46050-70ff-45b3-adec-5d98a11a5637"},{"version":"CommandV1","origId":17383912,"guid":"a89ad5ec-e671-4e47-8d7e-d692e2366c2b","subtype":"command","commandType":"auto","position":19.5,"command":"%md\n## Silver Table: Parsed Recording Data\n\nThe first of our silver tables will subscribe to the `recordings` dataset in the multiplex table and parse the JSON payload. The logic here is intended to just parse our JSON payload, which will enforce that this data matches the defined schema and validate the data quality of the recordings data.","commandVersion":63,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"98c348f0-a230-4e3d-8419-6c02a50101c2"},{"version":"CommandV1","origId":17383913,"guid":"1e9c254d-df34-4df9-810c-3d5bcbee30af","subtype":"command","commandType":"auto","position":19.625,"command":"(spark.readStream\n  .format(\"delta\")\n  .load(bronzePath)\n  .createOrReplaceTempView(\"bronze_unparsed_temp\"))","commandVersion":32,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002152837,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"eb154bfc-75f0-4ba9-90d8-0bc91a9b9594"},{"version":"CommandV1","origId":17383914,"guid":"4e3701cd-daf4-4caf-b00d-c74151eef6d5","subtype":"command","commandType":"auto","position":19.75,"command":"%sql\nCREATE OR REPLACE TEMP VIEW recordings_parsed_temp AS\n  SELECT json.device_id device_id, json.mrn mrn, json.heartrate heartrate, json.time time \n  FROM (\n    SELECT from_json(data, \"device_id INTEGER, mrn LONG, heartrate DOUBLE, time DOUBLE\") json\n    FROM bronze_unparsed_temp\n    WHERE dataset = \"recordings\")","commandVersion":108,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002156307,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"511bbd3c-75cb-4c1f-acce-2695c16a2f73"},{"version":"CommandV1","origId":17383915,"guid":"1659e71e-43c1-46d1-864d-55f187639809","subtype":"command","commandType":"auto","position":19.875,"command":"(spark.table(\"recordings_parsed_temp\")\n  .writeStream\n  .format(\"delta\")\n  .outputMode(\"append\")\n  .option(\"checkpointLocation\", recordingsParsedCheckpoint)\n  .start(recordingsParsedPath))","commandVersion":85,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002159665,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"92f40b36-8323-483e-9a96-821d6584cca6"},{"version":"CommandV1","origId":17383916,"guid":"75fb0d24-46fc-40a7-977a-e70c0ba014f9","subtype":"command","commandType":"auto","position":19.90625,"command":"%md\nAs new files arrived and are parsed into the upstream table, this query will automatically pick up those changes.","commandVersion":26,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2dae2af0-c8c0-4a41-b048-f0c4b3405a4e"},{"version":"CommandV1","origId":17383917,"guid":"530ec0dc-7885-4b9f-8c3b-86a7aa6eaf04","subtype":"command","commandType":"auto","position":19.921875,"command":"display(spark.readStream.format(\"delta\").load(recordingsParsedPath).groupBy().count())","commandVersion":2,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"018bb82f-5696-4b31-91b3-856a1c94d199"},{"version":"CommandV1","origId":17383918,"guid":"5dbc2d8e-5981-429f-bf23-55ab84f53031","subtype":"command","commandType":"auto","position":19.9375,"command":"NewFile.arrival()","commandVersion":2,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002186274,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"b7b37adc-1dae-4c94-a09a-ded0acc370b7"},{"version":"CommandV1","origId":17383919,"guid":"4d243e22-4c97-446a-8967-9102ddcee822","subtype":"command","commandType":"auto","position":20.0,"command":"%md\n### Load Static Personally Identifable Information (PII) Lookup Table\nThe ACID guarantees that Delta Lake brings to your data are managed at the table level, enforced as transactions complete and data is committed to storage. If you choose to merge these data with other data sources, be aware of how those sources version data and what sort of consistency guarantees they have.\n\nIn this simplified demo, we are loading a static CSV file to add patient data to our recordings. In production, we could use Databricks' [Auto Loader](https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html) feature to keep an up-to-date view of these data in our Delta Lake.","commandVersion":244,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"14f53258-76cc-446c-873d-e820cd20cf8b"},{"version":"CommandV1","origId":17383920,"guid":"f58ae572-6b1e-40e2-b7af-b8d8f3b21186","subtype":"command","commandType":"auto","position":21.0,"command":"(spark\n  .read\n  .format(\"csv\")\n  .schema(\"mrn STRING, name STRING\")\n  .option(\"header\", True)\n  .load(f\"{source_dir}/patient/patient_info.csv\")\n  .createOrReplaceTempView(\"pii\"))","commandVersion":41,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002189966,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"36b85763-6ee3-4239-9154-373b5d3d367d"},{"version":"CommandV1","origId":17383921,"guid":"9bd94355-c96c-4867-b557-7f199305e008","subtype":"command","commandType":"auto","position":21.5,"command":"%sql\nSELECT * FROM pii LIMIT 5","commandVersion":5,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002193472,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"1ed68231-7993-4c32-b115-0b0fa013f8ab"},{"version":"CommandV1","origId":17383922,"guid":"5cdb49db-9821-4bf9-863e-6ffe5fa092bb","subtype":"command","commandType":"auto","position":22.0,"command":"%md\n## Silver Table: Enriched Recording Data\nAs a second hop in our silver level, we will do the follow enrichments and checks:\n- Our recordings data will be joined with the PII to add patient names\n- The time for our recordings will be parsed to the format `'yyyy-MM-dd HH:mm:ss'` to be human-readable\n- We will exclude heart rates that are <= 0, as we know that these either represent the absence of the patient or an error in transmission","commandVersion":200,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2f6bd9df-c502-4f00-b735-55086418d49f"},{"version":"CommandV1","origId":17383923,"guid":"b46b5a43-8a83-4138-8fa8-c03195705aa5","subtype":"command","commandType":"auto","position":22.5,"command":"(spark.readStream\n  .format(\"delta\")\n  .load(recordingsParsedPath)\n  .createOrReplaceTempView(\"silver_recordings_temp\"))","commandVersion":32,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002199067,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"b0c2c9db-3880-499b-875d-ab666de06c8b"},{"version":"CommandV1","origId":17383924,"guid":"75512692-65ab-4e96-881d-c4937b1c0ffb","subtype":"command","commandType":"auto","position":22.75,"command":"%sql\nCREATE OR REPLACE TEMP VIEW recordings_w_pii AS (\n  SELECT device_id, a.mrn, b.name, cast(from_unixtime(time, 'yyyy-MM-dd HH:mm:ss') AS timestamp) time, heartrate\n  FROM silver_recordings_temp a\n  INNER JOIN pii b\n  ON a.mrn = b.mrn\n  WHERE heartrate > 0)","commandVersion":132,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002202657,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"6ed225af-4565-48ee-96e8-34eb07205960"},{"version":"CommandV1","origId":17383925,"guid":"16809ad6-7492-46ac-b277-59325a3269a7","subtype":"command","commandType":"auto","position":22.875,"command":"(spark.table(\"recordings_w_pii\")\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", recordingsEnrichedCheckpoint)\n  .outputMode(\"append\")\n  .start(recordingsEnrichedPath))","commandVersion":41,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002208137,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"59f9a718-4d99-40a4-905e-89acbf6b1e52"},{"version":"CommandV1","origId":17383926,"guid":"7101b47e-1fb6-44b6-94b7-69b1f7a2dcbb","subtype":"command","commandType":"auto","position":24.15625,"command":"%md\nTrigger another new file and wait for it propagate through both previous queries.","commandVersion":16,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"13680a77-2421-44b4-b596-e52fdcd9f0a5"},{"version":"CommandV1","origId":17383927,"guid":"e9479721-7709-4802-831f-a51aa4dbee13","subtype":"command","commandType":"auto","position":24.796875,"command":"display(spark.readStream.format(\"delta\").load(recordingsEnrichedPath).groupBy().count())","commandVersion":20,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002313204,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"0a5cd071-8cdc-4fa3-965d-73db71b45d4e"},{"version":"CommandV1","origId":17383928,"guid":"b8156401-7132-4fb1-a5ff-3ba5b8a7b2dd","subtype":"command","commandType":"auto","position":25.4375,"command":"NewFile.arrival()","commandVersion":2,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587002550762,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"589b7c0f-e98f-46c1-9f0a-75ef0a174aa6"},{"version":"CommandV1","origId":17383929,"guid":"fe2c78b1-8c8d-4d44-b5ba-9ebbb874c853","subtype":"command","commandType":"auto","position":28.0,"command":"%md\n## Gold Table: Daily Averages\n\nHere we read a stream of data from `recordingsEnrichedPath` and write another stream to create an aggregate gold table of daily averages for each patient.\n","commandVersion":29,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"5492d869-3d40-4cea-a1f3-0c754e03e057"},{"version":"CommandV1","origId":17383930,"guid":"f943bdc4-4855-4a47-8eea-d8878be0d00e","subtype":"command","commandType":"auto","position":28.75,"command":"(spark.readStream\n  .format(\"delta\")\n  .load(recordingsEnrichedPath)\n  .createOrReplaceTempView(\"recordings_enriched_temp\"))","commandVersion":32,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587011754425,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"baab5cdf-90db-441d-884b-cf09d62abc27"},{"version":"CommandV1","origId":17383931,"guid":"87f7e273-5de8-4d0b-ba20-34aa241c3661","subtype":"command","commandType":"auto","position":28.875,"command":"%sql\nCREATE OR REPLACE TEMP VIEW patient_avg AS (\n  SELECT mrn, name, MEAN(heartrate) avg_heartrate, date_trunc(\"DD\", time) date\n  FROM recordings_enriched_temp\n  GROUP BY mrn, name, date_trunc(\"DD\", time))","commandVersion":81,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587011776102,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"scatterPlot","width":"717.011364","height":"422.011364","xColumns":["mrn"],"yColumns":["day","avg(heartrate)"],"pivotColumns":[],"pivotAggregation":"sum","useConsistentColors":false,"customPlotOptions":{"barChart":[{"key":"grouped","value":true},{"key":"stacked","value":false},{"key":"100_stacked","value":false}],"mgLine":[{"key":"param","value":"{\"top\":20,\"bottom\":50,\"area\":false,\"missing_is_hidden\":true,\"legend_target\":\".dummy\",\"animate_on_load\":false,\"transition_on_update\":false,\"x_label\":\"day\",\"colors\":[\"#1f77b4\"]}"}],"scatterPlot":[{"key":"loess","value":false},{"key":"bandwidth","value":"0.3"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"dbbc4c3f-5d79-4b24-bfb8-383afe5515a1"},{"version":"CommandV1","origId":17383932,"guid":"eabfc993-58be-46d4-8b71-4be94cf30c86","subtype":"command","commandType":"auto","position":29.15625,"command":"%md\nNote that we're using `.trigger(once=True)` below. This provides us the ability to continue to use the strengths of structured streaming while trigger this job as a single batch. To recap, these strengths include:\n- exactly once end-to-end fault tolerant processing\n- automatic detection of changes in upstream data sources\n\nIf we know the approximate rate at which our data grows, we can appropriately size the cluster we schedule for this job to ensure fast, cost-effective processing. The customer will be able to evaluate how much updating this final aggregate view of their data costs and make informed decisions about how frequently this operation needs to be run.\n\nDownstream processes subscribing to this table do not need to re-run any expensive aggregations. Rather, files just need to be de-serialized and then queries based on included fields can quickly be pushed down against this already-aggregated source.","commandVersion":276,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"c2a6e1f7-0f6b-4ead-a467-b50676e03e3c"},{"version":"CommandV1","origId":17383933,"guid":"7bd37a79-3a4f-4a3b-81bb-0017f9ac81ba","subtype":"command","commandType":"auto","position":29.4375,"command":"(spark.table(\"patient_avg\")\n  .writeStream\n  .format(\"delta\")\n  .outputMode(\"complete\")\n  .option(\"checkpointLocation\", dailyAvgCheckpoint)\n  .trigger(once=True)\n  .start(dailyAvgPath)\n)","commandVersion":77,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587011787169,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"8ba92ccc-b4b1-4ff7-bf79-26492e7ae4f8"},{"version":"CommandV1","origId":17383934,"guid":"0d18a6a9-c719-4d99-bd64-f88eef326062","subtype":"command","commandType":"auto","position":30.0,"command":"%md\n### Register Daily Patient Averages Table to the Hive Metastore\n\nWe'll create an unmanaged table called `daily_patient_avg` using `DELTA`. This provides our BI analysts and data scientists easy access to these data.","commandVersion":56,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"06bce455-ed06-48fb-b892-969d5ca6eaa2"},{"version":"CommandV1","origId":17383935,"guid":"8b67685c-51fa-4cbf-9a0a-1b0fce58fbe1","subtype":"command","commandType":"auto","position":31.0,"command":"spark.sql(\"\"\"\n  DROP TABLE IF EXISTS daily_patient_avg\n\"\"\")\nspark.sql(f\"\"\"\n  CREATE TABLE daily_patient_avg\n  USING DELTA\n  LOCATION '{dailyAvgPath}'\n\"\"\")","commandVersion":35,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587011965619,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2729245e-7cb0-4f10-a9f2-d7eadd68c6e4"},{"version":"CommandV1","origId":17383936,"guid":"6d7b3122-a7d0-4918-bd50-a899da79e975","subtype":"command","commandType":"auto","position":32.0,"command":"%md-sandbox\n#### Important Considerations for `complete` Output with Delta\n\nWhen using `complete` output mode, we rewrite the entire state of our table each time our logic runs. While this is ideal for calculating aggregates, we **cannot** read a stream from this directory, as Structured Streaming assumes data is only being appended in the upstream logic.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Certain options can be set to change this behavior, but have other limitations attached. For more details, refer to [Delta Streaming: Ignoring Updates and Deletes](https://docs.databricks.com/delta/delta-streaming.html#ignoring-updates-and-deletes).\n\nThe gold Delta table we have just registered will perform a static read of the current state of the data each time we run the following query.","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"b8aac1ef-78b9-4b22-afcd-559e16d17c6c"},{"version":"CommandV1","origId":17383937,"guid":"05a0817e-ab50-4c17-b4fc-f6b8ff12084e","subtype":"command","commandType":"auto","position":33.0,"command":"%sql\nSELECT * FROM daily_patient_avg","commandVersion":15,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587011968933,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"mgLine","width":"660","height":"435.011364","xColumns":["date"],"yColumns":["avg_heartrate"],"pivotColumns":["name"],"pivotAggregation":"sum","useConsistentColors":false,"customPlotOptions":{"barChart":[{"key":"grouped","value":true},{"key":"stacked","value":false},{"key":"100_stacked","value":false}],"mgLine":[{"key":"param","value":"{\"top\":20,\"bottom\":50,\"area\":false,\"missing_is_hidden\":true,\"legend_target\":\".dummy\",\"animate_on_load\":false,\"transition_on_update\":false,\"colors\":[],\"x_label\":\"date\"}"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2e3f8460-d6e8-42f5-b492-84de19b0d2b3"},{"version":"CommandV1","origId":17383938,"guid":"be7e3d65-dc65-4e29-a6d8-c1a9ab7d85a6","subtype":"command","commandType":"auto","position":38.0,"command":"%md\nNote the above table includes all days for all users. If the predicates for our ad hoc queries match the data encoded here, we can push down our predicates to files at the source and very quickly generate more limited aggregate views.","commandVersion":47,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"654dcb18-3908-45bd-8a2d-1420cd473f9a"},{"version":"CommandV1","origId":17383939,"guid":"10f5dba2-7faf-46e2-822f-517ef670e76c","subtype":"command","commandType":"auto","position":40.5,"command":"%sql\nSELECT * \nFROM daily_patient_avg\nWHERE date BETWEEN \"2020-01-17\" AND \"2020-01-31\"","commandVersion":54,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587011988054,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"mgLine","width":"660","height":"auto","xColumns":["date"],"yColumns":["avg_heartrate"],"pivotColumns":["name"],"pivotAggregation":"sum","useConsistentColors":false,"customPlotOptions":{"mgLine":[{"key":"param","value":"{\"top\":20,\"bottom\":50,\"area\":false,\"missing_is_hidden\":true,\"legend_target\":\".dummy\",\"animate_on_load\":false,\"transition_on_update\":false,\"x_label\":\"date\",\"colors\":[\"#1f77b4\"]}"}]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"38be6003-6671-4114-99c6-43ab719778d6"},{"version":"CommandV1","origId":17383940,"guid":"a3bb4ee0-c972-48a8-8156-70c81b18844f","subtype":"command","commandType":"auto","position":41.75,"command":"%md\n## Process Remaining Records\nThe following cell will land additional files for the rest of 2020 in your source directory. You'll be able to see these process through the first 3 tables in your Delta Lake, but will need to re-run your final query to update your `daily_patient_avg` table, since this query uses the trigger once syntax.","commandVersion":77,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"f4d35361-e5c3-4682-a8e4-4ed3c5bcab7b"},{"version":"CommandV1","origId":17383941,"guid":"8275b9da-084f-4bdc-84a7-8acf55ad4634","subtype":"command","commandType":"auto","position":42.375,"command":"NewFile.arrival(continuous=True)","commandVersion":9,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587012029902,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"095fd2b4-f27f-4243-abe2-24c6424984a7"},{"version":"CommandV1","origId":17383942,"guid":"60e78f1e-4bdd-43bb-9b56-3a455b8cfa67","subtype":"command","commandType":"auto","position":43.0,"command":"%md\n## Wrapping Up\n\nFinally, make sure all streams are stopped.","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"548e4e34-5fc8-43bf-81b1-7c37c7499e3a"},{"version":"CommandV1","origId":17383943,"guid":"f0b8fb2e-93f8-4e57-8f15-7f97a8a18393","subtype":"command","commandType":"auto","position":44.0,"command":"for s in spark.streams.active:\n    s.stop()","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587012036103,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"doug.bateman@databricks.com","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"b2a7e08e-0d53-429c-bbb8-ffcb242072de"},{"version":"CommandV1","origId":17383944,"guid":"e7e0d12a-0673-4034-9cec-7018a78e8b5c","subtype":"command","commandType":"auto","position":45.0,"command":"%md\n## Summary\n\nDelta Lake is ideally suited for use in streaming data lake contexts.\n\nUse the Delta Lake architecture to craft raw, query and summary tables to produce beautiful visualizations of key business metrics.","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"373a5c18-cb89-498f-bb58-5f820c212aa7"},{"version":"CommandV1","origId":17383945,"guid":"7727060b-c0c6-4a32-bc2e-c68e69ace660","subtype":"command","commandType":"auto","position":46.0,"command":"%md\n## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/delta/delta-streaming.html#as-a-sink\" target=\"_blank\">Delta Streaming Write Notation</a>\n* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#\" target=\"_blank\">Structured Streaming Programming Guide</a>\n* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tagatha Das. This is an excellent video describing how Structured Streaming works.\n* <a href=\"http://lambda-architecture.net/#\" target=\"_blank\">Lambda Architecture</a>\n* <a href=\"https://bennyaustin.wordpress.com/2010/05/02/kimball-and-inmon-dw-models/#\" target=\"_blank\">Data Warehouse Models</a>\n* <a href=\"https://people.apache.org//~pwendell/spark-nightly/spark-branch-2.1-docs/latest/structured-streaming-kafka-integration.html#\" target=\"_blank\">Reading structured streams from Kafka</a>\n* <a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#creating-a-kafka-source-stream#\" target=\"_blank\">Create a Kafka Source Stream</a>\n* <a href=\"https://docs.databricks.com/delta/delta-intro.html#case-study-multi-hop-pipelines#\" target=\"_blank\">Multi Hop Pipelines</a>","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"9a29907a-0c78-4344-be87-00212520da1e"},{"version":"CommandV1","origId":17383946,"guid":"f6b44b5e-7ea6-4772-810c-638798a73876","subtype":"command","commandType":"auto","position":47.0,"command":"%md-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>","commandVersion":1,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"f61a6f99-4550-430e-99bf-91718b9a4648"}],"dashboards":[],"guid":"9c443db3-2496-43af-ba6c-07c086de424e","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}}